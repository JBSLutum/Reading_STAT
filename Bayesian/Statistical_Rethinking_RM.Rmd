---
title: "Statistical_Rethinking_RM"
author: "SM"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: true 
    toc_float: true
    toc_depth: 3  
    number_sections: true  
    theme: united  
    highlight: tango  
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 1: The Golem of Prague

$~$

Chapter 1 urged us to rethink the "popular statistical and scientific philosophy" by pointing out the flaws in classical statistical tools and reasoning. 

According to the author, "classical tools are not diverse enough to handle many common research questions...And if we keep adding new types of tools, soon there will be far too many to keep track of." I agree with these statements. The author also talked about how Fisher's exact test is used "whenever the cell counts are small" and he had "never seen it used appropriately" except Fisher's original use of the test. It would be nice if the author gave some examples and explanations of those inappropriate uses- I can't decide whether to accept without more information. 

The author mentioned why deductive falsification is impossible in scientific context:

(1) "Hypotheses are not models. The relations among hypotheses and different kinds of models are complex. Many models correspond to the same hypothesis, and many hypotheses correspond to a single model. This makes strict falsification impossible.

(2) Measurement matters. Even when we think the data falsify a model, another observer will debate our methods and measures. They don’t trust the data. Sometimes they are right.

For both of these reasons, deductive falsification never works. The scientific method cannot be reduced to a statistical procedure, and so our statistical methods should not pretend." 

$~$

## Hypotheses are not models

He gave an example of the first statement in evolutionary context. His conclusion was:

(1) "Any given statistical model (M) may correspond to more than one process model (P).

(2) Any given hypothesis (H) may correspond to more than one process model (P). 

(3) Any given statistical model (M) may correspond to more than one hypothesis (H)."

In other words, it doesn't matter whether we reject or fail to reject the null. It won't have "much inferential power" or give any useful insights due to possible existence of models- other than the ones under study- which can also align with the null or alternative hypotheses. The author gave a solution to this dilemma- "explicitly compare predictions of more than one model." This is a very nice solution, as I think it will allow analysis of hypothesis or whatever you want to study in real-world conditions. And that is much more powerful than any classical tests. 

In that case, should we disregard all studies that rely on NHST? Should we reject all findings made by rejecting the null hypothesis as senseless? And are the multiple process models the only way to go? The frequentists' way to solve this problem is to control the conditions of the experiment/study. In nutrition studies, this can be in the form of inclusion or exclusion criteria and strict control of test subjects (and conditions) throughout the duration of the study. This is to ensure the observed effect (or lack of) is caused by (or related to) the variables of interest and not by uncontrolled confounders. If a NHST research was done well with proper control, then we could 'more or less' have confidence in the research's findings. 

$~$

## *Measurement matters*

In the section "measurement matters," the author reinforced the first statement, and explained the fallacy of falsification principle, stating two reasons:

1. "Observations are prone to error, especially at the boundaries of scientific knowledge. 

2. Most hypotheses are quantitative, concerning the degrees of existence, rather than discrete, concerning total presence or absence."

### *Observation error*

The author gave two examples of observation error. The first example pointed out possible bias or errors in observations, and potential type 1 and type 2 errors (false positives and false negatives) in classical frequentist researches. And the second example talked about errors in measurements (or studies/ experiments). Both are valid concerns and those errors are not uncommon in frequentist researches. 

There are several ways to overcome these shortcomings, depending on research design. For example, we can increase sample size or run more tests to reduce type 1 and type 2 errors. Errors in measurements can be avoided by following protocols- taking at least two measurements for each sample, consistency in measurements, etc. However, there is no guarantee that any particular researcher or research group will follow a strict protocol without compromising, and that can lead to observation error. To prevent this, we usually avoid using finding from a single research, and wait till the findings are backed by systematic reviews and made into guidelines. 

In both woodpecker and neutrino examples, the author's key concerns were "whether falsification is real or spurious," and that "both true-detection and false-detection plausible." The null hypothesis in the woodpecker example was "The Ivory-billed Woodpecker is extinct." According to the author, it will only take a single woodpecker to falsify this theory, and that the 2005 evidence on existence of a woodpecker was doubted by many. Since no other evidences emerged, the question remained unresolved. That means finding was not conclusive yet, and research was still ongoing. As I mentioned above, we don't accept findings from a single research as concrete or irrefutable evidence. Thus, the dilemma over falsification and type 1 or type 2 error in the woodpecker case was not really a concern- we would just need to conduct more surveys (wider location for longer period?) or simply wait for new evidences. However, I think this is not a good case to use the null hypothesis testing. The falsification of hypothesis in this case will be simply the result of patience or luck, rather than proper research design or statistical tests. In the neutrino example, the problem was the human error which they solved by replication of experiments. This is not unusual in frequentist research. Replicating or reconducting experiment is a common occurrence, especially when lab analyses are involved. It may be an inconvenience, but it does not imply the utter failure of the theory of falsification or the null hypothesis testing. 

### *Continuous hypotheses*

The author noted "...it is a good practice to design experiments and observations that can differentiate competing hypotheses. But in many cases, the comparison must be probabilistic, a matter of degree, not kind." Two examples of null hypotheses that support the above statement were given: "80% of swans are white" and "Black swans are rare." The author pointed out the *modus tollens* swan story would not be applicable in this case, since the problem is not to prove or disprove the hypothesis but to estimate the distribution of swan coloration. He added "You might object that the hypothesis above is just not a good scientific hypothesis, because it isn’t easy to disprove. But if that’s the case, then most of the important questions about the world are not good scientific hypotheses."

In my opinion, it is not difficult to disprove both hypotheses. The two hypothese are not good hypotheses simply because they are not specific and fail to form sensible research question or outcome- not because it is "a matter of degree." Let's break down the two hypotheses. First, we think about the possible research question for the hypothesis "80% of swan are white." The research question could be "Are 80% of swan white?" or "Do 80% of swan have white feathers?" In this case, this hypothesis would be disprove if survey found any percent of white swan other than 80%. It wouldn't matter if the actual percent was 90 or 95- we could stop the research once the number passed the threshold of 80%. Now it is clear that the problem with this hypothesis is the pointless outcome. What are we trying to achieve knowing whether 80% of swan are white, or 40% or any percentage of swan are white in color? A more sensible hypothesis would be "White coloration is the dominant trait in swan." We can falsify this hypothesis by conducting surveys- like the author suggested- or genetic analysis, etc. Still, it is not a good hypothesis for an original research since the scope is too broad. The hypothesis only specify swan- not a particular speices or geographical location, meaning all swan species in the world are included in the research. That would be a hard task for any single researcher or research group. Hence, this hypothesis is more suitable at review level- not original research level. For original research, we can specify the species of swan and extent of study area based on the available funding.

The second hypothesis "Black swans are rare" has the similar problem- lack of specificity. We need to determine what is considered rare- 1 in 100 or 1 in a million? Once we have that specification, it is not difficult to tackle the second hypothesis. But we need to be aware that the second hypothesis also includes all swan species in the world- or even universe or parallel universes. 

Another thing is the *modus tollens* swan story was merely to explain a theory- not a definitive example of how we should write hypothesis. I have yet to see a research with hypothesis written like the one in *modus tollens* swan story. All hypotheses that I have seen so far address "the degree, not kind"-like the author suggested. Hence, the author's concern over hypotheses is somehow invalid. 

### *Falsification is consensual*

The author said "falsification is always consensual, not logical," and "evidence often— but not always— has something to do with such falsification." He also highlighted the arguments towards "consensus about meaning of evidence" and the danger of "historical revisionism." I think the author is right. We hold such reverence towards the famous scientists in history, that their formulas are still widely applied to this day, without contesting their suitability in this age. Many researches were done following the same mould, that they gave similar results, which became our gold-standard- the guidelines. And those guidelines are not set in stone. They are regularly updated- albeit minor changes. Occasionally, some guidelines are completely proven false and replaced by new ones. These occasions are rare that they are called groundbreaking discovery. I remembered the day everyone at the department was talking about the guideline change on egg consumption- may seem insignificant to outsiders but we were really excited! It could be that such new discoveries are rare because we are doing the same thing over and over again, and are content when our findings (even vaguely) agree the current consensus- we seek validation from previous works by researchers more venerable than us. And that is dangerous to our patients and the public. We may be feeling relaxed because we think the worst that can happen from such findings is "the treatment is ineffective but not harmful." We pour scorn on some of the treatments given by the healthcare providers some hundred years ago as harmful and ineffective. We can do so because we now have the knowledge collected over time. It is very possible our best guidelines may seem 'folly' to the researchers in the next hundred years. Thus, I agree with the author that it's time we should do things differently. 

$~$

## Three tools for golem engineering 


The author gave alternative to falsification- that is to build models. According to the author, "Models can be made into testing procedures— all statistical tests are also models—but they can also be used to measure, forecast, and argue." He also mentioned three models that are "in demand in both the social and biological sciences."

(1) Bayesian data analysis

(2) Multilevel models

(3) Model comparison using information criteria


### *Bayesian data analysis*

As explained by the author, Bayesian data analysis uses uncertainty or chance to "describe plausibility of different possibilities," and "...we can use probability theory as a general way to represent plausibility, whether in reference to countable events in the world or rather theoretical constructs like parameters." The author then compared this approach to frequentist approach- "Bayesian probability is a very general approach to probability...The frequentist approach requires that all probabilities be defined by connection to countable events and their frequencies in very large samples." And the author pointed out "...This leads to frequentist uncertainty being premised on imaginary resampling of data— if we were to repeat the measurement many many times, we would end up collecting a list of values that will have some pattern to it. It means also that parameters and models cannot have probability distributions, only measurements can. The distribution of these measurements is called a sampling distribution. This resampling is never done, and in general it doesn’t even make sense..." 

He also quoted "one of the most important frequentist statisticians of the 20^th^ century," Sir Ronald Fisher:

"[...] the only populations that can be referred to in a test of significance have no objective reality, being exclusively the product of the statistician’s imagination [...]"

But, Fisher's criticism was already rebuked by his contemporary Professor Egon Pearson in his ["Statistical Concepts in Their Relation to Reality"](https://errorstatistics.files.wordpress.com/2021/02/pearson_1955-stat-concepts-reality.pdf). Pearson said:

"...If probability is to be justly applied to the analysis of data, it follows that a random process must have been introduced or been naturally present at some stage in the collection of these data. Is there not then an appeal to the imagination in taking as the hypothetical population of samples that which would have been generated by repetition of this random process?..."

According to Pearson, Fisher was "often tilting at views which those whom he attacks have never held." And I can use Pearson's argument to address the "telescope" example given by the author.

"...Since the telescope was primitive, it couldn’t really focus the image very well. Saturn always appeared blurred. This is a statistical problem, of a sort. There’s uncertainty about the planet’s shape, but notice that none of the uncertainty is a result of variation in repeat measurements. We could look through the telescope a thousand times, and it will always give the same blurred image (for any given position of the Earth and Saturn). So the sampling distribution of any measurement is constant, because the measurement is deterministic— there’s nothing “random” about it. Frequentist statistical inference has a lot of trouble getting started here..."

I think, like Fisher, the author is also "tilting at views" which frequentists have never held. The problem with the telescope example is that repeated measurements were done on "ONE" sample only. That's why he made assumption that "...none of the uncertainty is a result of variation in repeat measurements...So the sampling distribution of any measurement is constant, because the measurement is deterministic— there’s nothing “random” about it." I have yet to see a research that only measures "one" sample repeatedly to solve a research question at population level or in real-world situations. It would be akin to asking a single child- using different sets of questions- repeatedly about what the child eats in a day to know the dietary pattern of all the children in the region. It makes no sense. Furthermore, sample representativeness play a major role in such researches. The author representation of a frequentist research has no place for sample representativeness- there is only one sample and nothing else. 

The author also criticized about "imagined repeat sampling." If taken out of context, such idea can be quite preposterous. But, it can actually be logical if we explain it in terms of sample representativeness. We are not imagining repeated sampling- beyond our chosen sample sets- to justify the results from our samples are applicable to the population. For example, if we were to study the dietary pattern of an ethnic group reside in the mountain, we do not need to ask dietary-related questions to every individual in the group. We select a fraction of the group- chosen at random- to infer the dietary pattern of the whole group. Such fraction (samples) should be high enough in number and have traits similar to the whole group. If results obtained from samples said "fish constitutes only a small part of their diet," then we can assume that we will have the same result if we did the same survey repeatedly with different sample sets obtained from the group. In this case, the so-called "imagined repeat sampling" can work because samples are representative of the population.  

The author continued in the next paragraph that "..even when a Bayesian procedure and frequentist procedure give exactly the same answer, our Bayesian golems aren’t justifying their inferences with imagined repeat sampling." Such narrative can give a result totally opposite of what the author wants. The author wants the readers to "rethinking statistics," by pointing out flaws in the classical statistic tests and research methodology. However, all the examples given by the author can be easily rebuked. More rebuttals will come if such narrative is introduced in a frequestist community- like academics in the nutrition field. But, these academics are not really protecting Karl Popper or E.S. Pearson. They are not defensive about the theories postulated years ago by scientists in the past. They are simply protecting their works- their findings. Majority- if not all- of these academics will not "rethink statistics" if Bayesian approach begins with denouncing the frequentist approach, especially when counter-arguments can easily be made. It will be similar to the author stabbing them with a knife first before offering a treatment. All or most of them will not accept the treatment. They will run away- either scared or hateful. Some may even stab him back. Even if they were forced to accept it, the acceptance would be superficial. Instead, we should offer them beautiful roses with exquisite scent- meaning we should focus on what Bayesian analysis can offer which frequentist apporach can't, rather than attacking them. The thorns may still prick their fingers, but they will not be able to resist the charm of the roses. And we can offer them balm to soothe their fingers. They will be thankful, and meaningful collaborations will be made.   

### *Multilevel models*

The author introduced the concept of multilevel model in this section. Statistical models "contain parameters. And parameters support inference. Upon what do parameters themselves stand? Sometimes, in some of the most powerful models, it’s parameters all the way down. What this means is that any particular parameter can be usefully regarded as a placeholder for a missing model. Given some model of how the parameter gets its value, it is simple enough to embed the new model inside the old one. This results in a model with multiple levels of uncertainty, each feeding into the next—a multilevel model."

The author also mentioned "four typical and complementary reasons to sue multilevel models:"

(1) ***To adjust estimates for repeat sampling.*** When more than one observation arises from the same individual, location, or time, then traditional, single-level models may mislead us.

(2) ***To adjust estimates for imbalance in sampling.*** When some individuals,locations,or times are sampled more than others, we may also be misled by single-level models.

(3) ***To study variation.*** If our research questions include variation among individuals or other groups within the data, then multilevel models are a big help, because they model variation explicitly.

(4) ***To avoid averaging.*** Frequently, scholars pre-average some data to construct variables for a regression analysis. This can be dangerous, because averaging removes variation. It therefore manufactures false confidence. Multilevel models allow us to preserve the uncertainty in the original, pre-averaged values, while still using the average to make predictions.

And there are also "diverse model types that turn out to be multilevel: models for missing data (imputation), measurement error, factor analysis, some time series models, types of spatial and network regression, and phylogenetic regressions all are special applications of the multilevel strategy."

The author added "this is why grasping the concept of multilevel modeling may lead to a perspective shift. Suddenly single- level models end up looking like mere components of multilevel models. The multilevel strategy provides an engineering principle to help us to introduce these components into a particular analysis, exactly where we think we need them."

Based on the author's explanation, multilevel models sound really promising, and I'm eager to learn more about them.

### *Model comparison and information criteria*

Information criteria were developed to compare "structurally different models based upon future predictive accuracy, using different approximations for different model types."

Popular information criterion AIC, and related metrics- such as DIC and WAIC- "explicitly build a model of the prediction task and use that model to estimate performance of each model you might wish to compare. Because the prediction is modeled, it depends upon assumptions. So information criteria do not in fact achieve the impossible, by seeing the future. They are still golems." The author also mentioned that they are known as "information criteria, because they develop their measure of model accuracy from information theory."

These criteria can help researchers with two common difficulties in model comparison:

(1)  ***Overfitting.*** Future data will not be exactly like past data, and so any model that is unaware of this fact tends to make worse predictions than it could. So if we wish to make good predictions, we cannot judge our models simply on how well they fit our data. Information criteria provide estimates of predictive accuracy, rather than merely fit. So they compare models where it matters.

(2) ***Comparison of multiple non-null models to the same data.*** Frequently, several plausible models of a phenomenon are known to us. In some empirical contexts, like social networks and evolutionary phylogenies, there are no reasonable or uniquely “null” models. This was also true of the neutral evolution example. In such cases, it’s not only a good idea to explicitly compare models. It’s also mandatory. Information criteria aren’t the only way to conduct the comparison. But they are an accessible and widely used way.

$~$

# Lecture 1: Golem of Prague

$~$

According to the lecture, 

<span style="color:brown"><span style="text-decoration:underline">**Research requires:**</span>

- Precise process model(s)

- Statistical model (procedure, golem) justified by implications of process model(s) and question (estimand)

$~$

<span style="color:brown"><span style="text-decoration:underline">**Three modes to draw the Bayesian Owl:**</span>

1. Understand what you are doing 

2. Document your work, reduce error

3. Respectable scientific workflow

$~$

<span style="color:brown"><span style="text-decoration:underline">**Outline of a scientific workflow**</span>

1. Theoretical estimand
    - to have a clear idea of what you are trying to do in the first place

\n

2. Scientific (causal) model(s)
    - to precisely define theoretical estimand
    - simulating model or logical model that generate synthetic observations (can produce data) &rarr; let us design statistical procedures
    
\n

3. Use 1 & 2 to build statistical model(s)

4. Simulate from 2 to validate 3 yields 1
    - to justify or check our workflow to ensure the software works

\n

5. Analyze real data

$~$

<span style="color:brown"><span style="text-decoration:underline">**Advantages of Bayseian approach**</span>

- Permissive, flexible

- Express uncertainty at all levels

- Direct solutions for measurement error, missing data

- Focus on scientific modeling 

$~$

<span style="color:brown"><span style="text-decoration:underline">**Science before Statistics**</span>

- For *statistical models* to produce scientific insights, they require additional *scientific (causal) models*

- The *reasons* for a statistical analysis are not found in the data themselves, but rather in the *causes* of the data

- The *causes* of the data cannot be extracted from the data alone. *No causes in; no causes out.*

$~$

<span style="color:brown"><span style="text-decoration:underline">**Causes are not optional**</span>

- Even when goal is *descriptive*, need causal model

- The *sample* differs from the *population*; describing the population requires causal thinking

$~$

<span style="color:brown"><span style="text-decoration:underline">**What is Causal Inference?**</span>

- More than *association* between variables

- Causal inference is *prediction* of intervention

- Causal inference is *imputation* of missing observations

<span style="color:purple">***Causal Prediction***</span>

- Knowing a *cause* means being able to predict the *consequences* of an *intervention*. *What if I do this?*

<span style="color:purple">***Causal Imputation***</span>

- Knowing a *cause* means being able to construct unobserved *counterfactual outcomes*. *What if I had done something else?*

$~$

<span style="color:brown"><span style="text-decoration:underline">**Directed Acyclic Graphs (DAGs)**</span> 

- Heuristic causal models
  - Transparent scientific assumptions to *justify* scientific effort, *expose* it to useful critique, and *connect* theories to golems- brainless, powerful statistical models

\n

- Clarify scientific thinking

- Analyze to deduce appropriate statistical models

<span style="color:purple">***Why DAGs?***</span>

- Different queries, different models

- Which control variables?

- Absolute not safe to add everything- ***bad controls***

- How to test the causal model?

- With more scientific knowledge, can do more

$~$

# Chapter 2: Small Worlds and Large Worlds

$~$

## The garden of forking data


"In order to make good inference about what actually happened, it helps to consider everything that could have happened. A Bayesian analysis is a garden of forking data, in which alternative sequences of events are cultivated. As we learn about what did happen, some of these alternative sequences are pruned. In the end, what remains is only what is logically consistent with our knowledge."

### Counting possibilities

<span style="color:green">***A bag of avocado and pear (total 8)***</span>

- Conjectures: (a,a,a,a,a,a,a,a); (p,a,a,a,a,a,a,a); (p,p,a,a,a,a,a,a); (p,p,p,a,a,a,a,a); (p,p,p,p,a,a,a,a);
               (p,p,p,p,p,a,a,a); (p,p,p,p,p,p,a,a); (p,p,p,p,p,p,p,a); (p,p,p,p,p,p,p,p)

<span style="color:green">***Shake the bag to give equal chance and pick 4 fruits; replace the fruit after each pick***</span>

- Data: (a,p,a,a)

<span style="color:green">***Count the possible ways to have that exact data sequence for each conjecture***</span>

- (a,a,a,a,a,a,a,a)       &rarr;    8 * 0 * 8 * 8 = 0

- (p,a,a,a,a,a,a,a)       &rarr;    7 * 1 * 7 * 7 = 343         

- (p,p,a,a,a,a,a,a)       &rarr;    6 * 2 * 6 * 6 = 432

- (p,p,p,a,a,a,a,a)       &rarr;    5 * 3 * 5 * 5 = 375

- (p,p,p,p,a,a,a,a)       &rarr;    4 * 4 * 4 * 4 = 256
               
- (p,p,p,p,p,a,a,a)       &rarr;    3 * 5 * 3 * 3 = 135

- (p,p,p,p,p,p,a,a)       &rarr;    2 * 6 * 2 * 2 = 48

- (p,p,p,p,p,p,p,a)       &rarr;    1 * 7 * 1 * 1 = 7

- (p,p,p,p,p,p,p,p)       &rarr;    0 * 8 * 0 * 0 = 0


"So what good are these counts? By comparing these counts, we have part of a solution for a way to rate the relative plausibility of each conjectured bag composition. But it’s only a part of a solution, because in order to compare these counts we first have to decide how many ways each conjecture could itself be realized. We might argue that when we have no reason to assume otherwise, we can just consider each conjecture equally plausible and compare the counts directly. But often we do have reason to assume otherwise."


### Using prior information 

Two possible choices to incorporate prior information: add a new layer to the garden or multiply prior count by the new count. "It turns out that these two methods are mathematically identical, as long as the new observation is logically independent of the previous observations."

"First we count the numbers of ways each conjecture could produce the new observation. Then we multiply each of these new counts by the previous numbers of ways for each conjecture."

We select one more fruit from the bag. Now the data sequence is "a,p,a,a,p"

- (a,a,a,a,a,a,a,a)       &rarr;       0 * 0 = 0

- (p,a,a,a,a,a,a,a)       &rarr;     343 * 1 = 343            

- (p,p,a,a,a,a,a,a)       &rarr;     432 * 2 = 864

- (p,p,p,a,a,a,a,a)       &rarr;     375 * 3 = 1125

- (p,p,p,p,a,a,a,a)       &rarr;     256 * 4 = 1024
               
- (p,p,p,p,p,a,a,a)       &rarr;     135 * 5 = 675

- (p,p,p,p,p,p,a,a)       &rarr;      48 * 6 = 288

- (p,p,p,p,p,p,p,a)       &rarr;       7 * 7 = 49

- (p,p,p,p,p,p,p,p)       &rarr;       0 * 8 = 0

"This updating approach amounts to nothing more than asserting that (1) when we have previous information suggesting there are W~prior~ ways for a conjecture to produce a previous observation D~prior~ and (2) we acquire new observations D~new~ that the same conjecture can produce in W~new~ ways, then (3) the number of ways the conjecture can account for both D~prior~ as well as D~new~ is just the product W~prior~ × W~new~."

$~$

<span style="color:brown"><span style="text-decoration:underline">**Incorporating new data types different from prior data**</span>

*New information: There are fewer pears than avocados. For every bag containing 5 pears and 3 avocados, there are two bags containing 4 pears and 4 avocados, three bags containing 5 avocados and 3 pears, and four bags containing 2 pears and 6 avocados. Each bag contains at least 2 pears and 2 avocados.*

- (a,a,a,a,a,a,a,a)       &rarr;       0 * 0 = 0

- (p,a,a,a,a,a,a,a)       &rarr;     343 * 0 = 0            

- (p,p,a,a,a,a,a,a)       &rarr;     864 * 4 = 3456

- (p,p,p,a,a,a,a,a)       &rarr;    1125 * 3 = 3375

- (p,p,p,p,a,a,a,a)       &rarr;    1024 * 2 = 2048
               
- (p,p,p,p,p,a,a,a)       &rarr;     675 * 1 = 675

- (p,p,p,p,p,p,a,a)       &rarr;     288 * 0 = 0

- (p,p,p,p,p,p,p,a)       &rarr;      49 * 0 = 0

- (p,p,p,p,p,p,p,p)       &rarr;       0 * 0 = 0


I made a mental note in the beginning that the bag contained 4 pears and 4 avocados. Now, using the counting method has gotten me nearer to the truth, but it makes no difference since I will get the wrong answer anyway. The author asked "Is there a threshold difference in these counts at which we can safely decide that one of the conjectures is the correct one?", when one conjecture is barely more possible than the other. I think the answer is no (correct answer is in chapter 3). 

"Which assumption should we use, when there is no previous information about the conjectures? The most common solution is to assign an equal number of ways that each conjecture could be correct, before seeing any data. This is sometimes known as the *principle of indifference*: When there is no reason to say that one conjecture is more plausible than another, weigh all of the conjectures equally. The issue of choosing a representation of “ignorance” is surprisingly complicated. The issue will arise again in later chapters. For the sort of problems we examine in this book, the principle of indifference results in inferences very comparable to mainstream non- Bayesian approaches, most of which contain implicit equal weighting of possibilities. For example a typical non-Bayesian confidence interval weighs equally all of the possible values a parameter could take, regardless of how implausible some of them are. Many non-Bayesian procedures have moved away from this, through the use of penalized likelihood and other methods."

$~$

### From counts to probability











    




























































---
