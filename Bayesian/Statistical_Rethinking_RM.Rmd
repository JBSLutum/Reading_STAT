---
title: "Statistical_Rethinking_RM"
author: "SM"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: true 
    toc_float: true
    toc_depth: 3  
    number_sections: true  
    theme: united  
    highlight: tango  
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 1: The Golem of Prague

$~$

Chapter 1 urged us to rethink the "popular statistical and scientific philosophy" by pointing out the flaws in classical statistical tools and reasoning. 

According to the author, "classical tools are not diverse enough to handle many common research questions...And if we keep adding new types of tools, soon there will be far too many to keep track of." I agree with these statements. The author also talked about how Fisher's exact test is used "whenever the cell counts are small" and he had "never seen it used appropriately" except Fisher's original use of the test. It would be nice if the author gave some examples and explanations of those inappropriate uses- I can't decide whether to accept without more information. 

The author mentioned why deductive falsification is impossible in scientific context:

(1) "Hypotheses are not models. The relations among hypotheses and different kinds of models are complex. Many models correspond to the same hypothesis, and many hypotheses correspond to a single model. This makes strict falsification impossible.

(2) Measurement matters. Even when we think the data falsify a model, another observer will debate our methods and measures. They don’t trust the data. Sometimes they are right.

For both of these reasons, deductive falsification never works. The scientific method cannot be reduced to a statistical procedure, and so our statistical methods should not pretend." 

## *Hypotheses are not models*

He gave an example of the first statement in evolutionary context. His conclusion was:

(1) "Any given statistical model (M) may correspond to more than one process model (P).

(2) Any given hypothesis (H) may correspond to more than one process model (P). 

(3) Any given statistical model (M) may correspond to more than one hypothesis (H)."

In other words, it doesn't matter whether we reject or fail to reject the null. It won't have "much inferential power" or give any useful insights due to possible existence of models- other than the ones under study- which can also align with the null or alternative hypotheses. The author gave a solution to this dilemma- "explicitly compare predictions of more than one model." This is a very nice solution, as I think it will allow analysis of hypothesis or whatever you want to study in real-world conditions. And that is much more powerful than any classical tests. 

In that case, should we disregard all studies that rely on NHST? Should we reject all findings made by rejecting the null hypothesis as senseless? And are the multiple process models the only way to go? The frequentists' way to solve this problem is to control the conditions of the experiment/study. In nutrition studies, this can be in the form of inclusion or exclusion criteria and strict control of test subjects (and conditions) throughout the duration of the study. This is to ensure the observed effect (or lack of) is caused by (or related to) the variables of interest and not by uncontrolled confounders. If a NHST research was done well with proper control, then we could 'more or less' have confidence in the research's findings. 

$~$

## *Measurement matters*

In the section "measurement matters," the author reinforced the first statement, and explained the fallacy of falsification principle, stating two reasons:

1. "Observations are prone to error, especially at the boundaries of scientific knowledge. 

2. Most hypotheses are quantitative, concerning the degrees of existence, rather than discrete, concerning total presence or absence."

### *Observation error*

The author gave two examples of observation error. The first example pointed out possible bias or errors in observations, and potential type 1 and type 2 errors (false positives and false negatives) in classical frequentist researches. And the second example talked about errors in measurements (or studies/ experiments). Both are valid concerns and those errors are not uncommon in frequentist researches. 

There are several ways to overcome these shortcomings, depending on research design. For example, we can increase sample size or run more tests to reduce type 1 and type 2 errors. Errors in measurements can be avoided by following protocols- taking at least two measurements for each sample, consistency in measurements, etc. However, there is no guarantee that any particular researcher or research group will follow a strict protocol without compromising, and that can lead to observation error. To prevent this, we usually avoid using finding from a single research, and wait till the findings are backed by systematic reviews and made into guidelines. 

In both woodpecker and neutrino examples, the author's key concerns were "whether falsification is real or spurious," and that "both true-detection and false-detection plausible." The null hypothesis in the woodpecker example was "The Ivory-billed Woodpecker is extinct." According to the author, it will only take a single woodpecker to falsify this theory, and that the 2005 evidence on existence of a woodpecker was doubted by many. Since no other evidences emerged, the question remained unresolved. That means finding was not conclusive yet, and research was still ongoing. As I mentioned above, we don't accept findings from a single research as concrete or irrefutable evidence. Thus, the dilemma over falsification and type 1 or type 2 error in the woodpecker case was not really a concern- we would just need to conduct more surveys (wider location for longer period?) or simply wait for new evidences. However, I think this is not a good case to use the null hypothesis testing. The falsification of hypothesis in this case will be simply the result of patience or luck, rather than proper research design or statistical tests. In the neutrino example, the problem was the human error which they solved by replication of experiments. This is not unusual in frequentist research. Replicating or reconducting experiment is a common occurrence, especially when lab analyses are involved. It may be an inconvenience, but it does not imply the utter failure of the theory of falsification or the null hypothesis testing. 

### *Continuous hypotheses*

The author noted "...it is a good practice to design experiments and observations that can differentiate competing hypotheses. But in many cases, the comparison must be probabilistic, a matter of degree, not kind." Two examples of null hypotheses that support the above statement were given: "80% of swans are white" and "Black swans are rare." The author pointed out the *modus tollens* swan story would not be applicable in this case, since the problem is not to prove or disprove the hypothesis but to estimate the distribution of swan coloration. He added "You might object that the hypothesis above is just not a good scientific hypothesis, because it isn’t easy to disprove. But if that’s the case, then most of the important questions about the world are not good scientific hypotheses."

In my opinion, it is not difficult to disprove both hypotheses. The two hypothese are not good hypotheses simply because they are not specific and fail to form sensible research question or outcome- not because it is "a matter of degree." Let's break down the two hypotheses. First, we think about the possible research question for the hypothesis "80% of swan are white." The research question could be "Are 80% of swan white?" or "Do 80% of swan have white feathers?" In this case, this hypothesis would be disprove if survey found any percent of white swan other than 80%. It wouldn't matter if the actual percent was 90 or 95- we could stop the research once the number passed the threshold of 80%. Now it is clear that the problem with this hypothesis is the pointless outcome. What are we trying to achieve knowing whether 80% of swan are white, or 40% or any percentage of swan are white in color? A more sensible hypothesis would be "White coloration is the dominant trait in swan." We can falsify this hypothesis by conducting surveys- like the author suggested- or gene expression analysis, etc. Still, it is not a good hypothesis for an original research since the scope is too broad. The hypothesis only specify swan- not a particular speices or geographical location, meaning all swan species in the world are included in the research. That would be a hard task for any single researcher or research group. Hence, this hypothesis is more suitable at review level- not original research level. For original research, we can specify the species of swan and extent of study area based on the available funding.

The second hypothesis "Black swans are rare" has the similar problem- lack of specificity. We need to determine what is considered rare- 1 in 100 or 1 in a million? Once we have that specification, it is not difficult to tackle the second hypothesis. But we need to be aware that the second hypothesis also includes all swan species in the world- or even universe or parallel universes. 

Another thing is the *modus tollens* swan story was merely to explain a theory- not a definitive example of how we should write hypothesis. I have yet to see a research with hypothesis written like the one in *modus tollens* swan story. All hypotheses that I have seen so far address "the degree, not kind"-like the author suggested. Hence, the author's concern over hypotheses is somehow invalid. 











































---
