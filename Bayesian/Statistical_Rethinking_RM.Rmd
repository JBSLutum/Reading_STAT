---
title: "Statistical_Rethinking_RM"
author: "SM"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: true 
    toc_float: true
    toc_depth: 3  
    number_sections: true  
    theme: united  
    highlight: tango  
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 1: The Golem of Prague

$~$

Chapter 1 urged us to rethink the "popular statistical and scientific philosophy" by pointing out the flaws in classical statistical tools and reasoning. 

According to the author, "classical tools are not diverse enough to handle many common research questions...And if we keep adding new types of tools, soon there will be far too many to keep track of." I agree with these statements. The author also talked about how Fisher's exact test is used "whenever the cell counts are small" and he had "never seen it used appropriately" except Fisher's original use of the test. It would be nice if the author gave some examples and explanations of those inappropriate uses- I can't decide whether to accept without more information. 

The author mentioned why deductive falsification is impossible in scientific context:

(1) "Hypotheses are not models. The relations among hypotheses and different kinds of models are complex. Many models correspond to the same hypothesis, and many hypotheses correspond to a single model. This makes strict falsification impossible.

(2) Measurement matters. Even when we think the data falsify a model, another observer will debate our methods and measures. They donâ€™t trust the data. Sometimes they are right.

For both of these reasons, deductive falsification never works. The scientific method cannot be reduced to a statistical procedure, and so our statistical methods should not pretend." 

## *Hypotheses are not models*

He gave an example of the first statement in evolutionary context. His conclusion was:

(1) "Any given statistical model (M) may correspond to more than one process model (P).

(2) Any given hypothesis (H) may correspond to more than one process model (P). 

(3) Any given statistical model (M) may correspond to more than one hypothesis (H)."

In other words, it doesn't matter whether we reject or fail to reject the null. It won't have "much inferential power" or give any useful insights due to possible existence of models- other than the ones under study- which can also align with the null or alternative hypotheses. The author gave a solution to this dilemma- "explicitly compare predictions of more than one model." This is a very nice solution, as I think it will allow analysis of hypothesis or whatever you want to study in real-world conditions. And that is much more powerful than any classical tests. 

In that case, should we disregard all studies that rely on NHST? Should we reject all findings made by rejecting the null hypothesis as senseless? And are the multiple process models the only way to go? The frequentists' way to solve this problem is to control the conditions of the experiment/study. In nutrition studies, this can be in the form of inclusion or exclusion criteria and strict control of test subjects (and conditions) throughout the duration of the study. This is to ensure the observed effect (or lack of) is caused by (or related to) the variables of interest and not by uncontrolled confounders. If a NHST research was done well with proper control, then we could 'more or less' have confidence in the research's findings. 

$~$

## *Measurement matters*

In the section "measurement matters," the author reinforced the first statement, and explained the fallacy of falsification principle, stating two reasons:

1. "Observations are prone to error, especially at the boundaries of scientific knowledge. 

2. Most hypotheses are quantitative, concerning the degrees of existence, rather than discrete, concerning total presence or absence."

### *Observation error*

The author gave two examples of observation error. The first example pointed out possible bias or errors in observations, and potential type 1 and type 2 errors (false positives and false negatives) in classical frequentist researches. And the second example talked about errors in measurements (or studies/ experiments). Both are valid concerns and those errors are not uncommon in frequentist researches. 

There are several ways to overcome these shortcomings, depending on research design. For example, we can increase sample size or run more tests to reduce type 1 and type 2 errors. Errors in measurements can be avoided by following protocols- taking at least two measurements for each sample, consistency in measurements, etc. However, there is no guarantee that any particular researcher or research group will follow a strict protocol without compromising, and that can lead to observation error. To prevent this, we usually avoid using finding from a single research, and wait till the findings are backed by systematic reviews and made into guidelines. 

In both woodpecker and neutrino examples, the author's key concerns were "whether falsification is real or spurious," and that "both true-detection and false-detection plausible." The null hypothesis in the woodpecker example was "The Ivory-billed Woodpecker is extinct." According to the author, it will only take a single woodpecker to falsify this theory, and that the 2005 evidence on existence of a woodpecker was doubted by many. Since no other evidences emerged, the question remained unresolved. That means finding was not conclusive yet, and research was still ongoing. As I mentioned above, we don't accept findings from a single research as concrete or irrefutable evidence. Thus, the dilemma over falsification and type 1 or type 2 error in the woodpecker case was not really a concern- we would just need to conduct more surveys (wider location for longer period?) or simply wait for new evidences. However, I think this is not a good case to use the null hypothesis testing. The falsification of hypothesis in this case will be simply the result of patience or luck, rather than proper research design or statistical tests. In the neutrino example, the problem was the human error which they solved by replication of experiments. This is not unusual in frequentist research. Replicating or reconducting experiment is a common occurrence, especially when lab analyses are involved. It may be an inconvenience, but it does not imply the utter failure of the theory of falsification or the null hypothesis testing. 

### *Continuous hypotheses*







































---
