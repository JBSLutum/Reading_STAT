---
title: "Statistical_Rethinking_Lecture_Homework_2019"
author: "SM"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: true 
    toc_float: true
    toc_depth: 3  
    number_sections: true  
    theme: united  
    highlight: tango  
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
library(ggplot2)
library(dplyr)
```

$~$

# Lecture 1: The Golem of Prague

$~$

<span style="color:brown"><span style="text-decoration:underline">**Bayesian data analysis**</span>

- Use probability to describe uncertainty
    - Extend ordinary logic (true/ false) to continuous plausibility
    - *Count all the ways data can happen, according to assumptions.*
    - *Assumptions with more ways that are consistent with data are more plausible.* 

\n

- Computationally difficult
    - Markov chain Monte Carlo (MCMC) to the rescue

\n

- Used to be controversial
    - Ronald Fisher: Bayesian analysis "must be wholly rejected."
    
$~$

<span style="color:brown"><span style="text-decoration:underline">**Multilevel models**</span>

- Models with multiple levels of uncertainty
    - Replace parameters with models
    
\n

- Common uses
    - Repeat & imbalanced sampling
    - Study variation
    - Avoid averaging
    - Phylogenetics, factor and path analysis, networks, spatial models
    
\n

- Natural Bayesian strategy

$~$

<span style="color:brown"><span style="text-decoration:underline">**Model comparison**</span>

- Instead of falsifying a null model, compare meaningful models

- Basic problems
    - ***Overfitting:*** most ways of training the model on data lead the model to really love your samples, not love the world (really good in encrypting samples you feed it, but won't necessarily good at making predictions)
    - ***Causal inference:*** need more than one models to figure out the network or given the network, test whether it is true or not
    
\n

- Information theory less silly
    - AIC, WAIC, cross-validation
    
\n

- Must distinguish prediction from inference

$~$

<span style="color:brown"><span style="text-decoration:underline">**Small and large worlds**</span>

- **Small world:** The world of the golem's assumptions. Bayesian golems are optimal, in the small world.

- **Large world:** The real world. No guarantee of optimality for any kind of golem.

- Have to worry about both. 

$~$

<span style="color:brown"><span style="text-decoration:underline">**Garden of forking data**</span>

- The future:
    - Full of branching paths
    - Each choice closes some
    
\n

- The data:
    - Many possible events
    - Each observation eliminates some
    
$~$

<span style="color:brown"><span style="text-decoration:underline">**Counts to plausibility**</span>

- Plausibility is probability: Set of non-negative real numbers that sum to one.

- Probability theory is just a set of shortcuts for counting possibilities

$~$

# Lecture 2: The Garden of Forking Data

$~$

<span style="color:brown"><span style="text-decoration:underline">**(1) Design**</span>

- Data story motivates the model
    - How do data arise?

\n

- For W L W W W L W L W:
    - Some true proportion of water, p
    - Toss globe, probability p of observing W, 1-p of L
    - Each toss therefore independent of other tosses

\n

- Translate data story into probability statements

$~$

<span style="color:brown"><span style="text-decoration:underline">**(2) Condition**</span>

- Bayesian updating defines optimal learning in a small world, converts prior into posterior
    - Give your golem an information state, before the data: Here, an initial confidence in each possible value of p between zero and one
    - Condition on data to update information state: New confidence in each value of p, conditional on data
    
\n

- Data order irrelevant, because golem assumes order irrelevant (because the tosses are independent, but this may not be true in other models)
    - All-at-once, one-at-a-time, shuffled order all give same posterior
    - *Still data has sequence and you should pay attention to that- maybe other models where order matter??*

\n

- Every posterior is a prior for next observation

- Every prior is posterior of some other inference

- Sample size automatically embodied in posterior 
    
$~$

<span style="color:brown"><span style="text-decoration:underline">**(3) Evaluate**</span>

- Bayesian inference: Logical answer to a question in the form of a model
    - *"How plausible is each proportion of water, given these data?"*
    
\n

- Golem must be supervised
    - Did the golem malfunction?
    - Does the golem's answer make sense?
    - Does the question make sense?
    - Check sensitivity of answer to changes in assumptions

$~$

<span style="color:brown"><span style="text-decoration:underline">**Construction perspective**</span>

- Build joint model:
    1. List variables (things you can observe, and things you can't observe but are needed to make inferences)
    2. Define generative relations (relationships among variables)
    3. ???
    4. Profit
    
\n

- Input: Joint prior (the prior probability of data and the parameters)

- Deduce: Joint posterior

$~$

<span style="color:brown"><span style="text-decoration:underline">**W distribution (Likelihood)**</span>

- The count of W's is distributed *binomially*, with probability p of a W on each toss and N tosses total.

$$Pr(W|N, p) = \frac{N!}{W!(N - W)!} p^W (1 - p)^{N-W}$$

```
W   = count W (count of water observation)
N   = number tosses
p   = probability W

```

$~$

<span style="color:brown"><span style="text-decoration:underline">**Prior probability p**</span>

- What the golem believes before the data arrive

- Equal prior probability 0-1 (non-informative/ uniform prior)

- Pr(W) & Pr(p) define *prior predictive distribution*

$~$

<span style="color:brown"><span style="text-decoration:underline">**Prior literature**</span>

- Huge literature on choice of prior

- Flat prior conventional and bad
    - Always know something (before data) that can improve inference
    - Are zero and one plausible values for p? Is p < 0.5 as plausible as p > 0.5?
    - There is no "true" prior
    - Just need to do better than flat
    
\n

- All above equally true of likelihood

$~$

<span style="color:brown"><span style="text-decoration:underline">**Joint model**</span>

A simplest Bayesian model: 

```
W ~ Binominal(N, p)

p ~ Uniform(0, 1)

```
$~$

<span style="color:brown"><span style="text-decoration:underline">**Posterior probability**</span>

- Bayesian "estimate" is always *posterior distribution over parameters*, Pr(parameters|data)

- Here: Pr(p|W, N)

- Compute using Bayes' Theorem:

$$Pr(p|W,N) = \frac{Pr(W|N,p)~Pr(p)}{\sum\Pr(W|N,p)~Pr(p)~for~all~p}$$

$~$


$$Posterior = \frac{(Prob~observed~variables)~*~(Prior)}{Normalizing~constant}$$

$~$

<span style="color:brown"><span style="text-decoration:underline">**Computing the posterior**</span>

1. Analytical approach (often impossible- not very useful, only works in a small number of models)

2. Grid approximation (very intensive- not very useful in general)

3. Quadratic approximation (limited)

4. Markov chain Monte Carlo (intensive)

$~$

<span style="color:brown"><span style="text-decoration:underline">**Grid approximation**</span>

- The posterior probability is: 

  *standardized product of*
  
  *(1) probability of data*
  
  *(2) prior probability*
  
\n

- "Standardized" means: Add up all the products and divide each by this sum

- Grid approximation uses *finite grid* of parameter values instead of continuous space

- Too expensive with more than a few parameters

$~$

<span style="color:brown"><span style="text-decoration:underline">**Sampling from the posterior**</span>

- Incredibly useful to sample *randomly* from the posterior
    - Visualize uncertainty
    - Compute confidence interval
    - Simulate observations
    
\n

- MCMC produces only samples

- Above all, *easier to think with samples*

- Transforms a hard calculus problem into an easy data summary problem

- Recipe:
    1. Compute or approximate posterior
    2. Sample with replacement from posterior
    3. Compute stuff from samples

$~$

<span style="color:purple">***Compute stuff***</span>

- Summary tasks
    - How much posterior probability below/ above/ between specified parameter values?
    - Which parameter values contain 50%/ 80%/ 95% of posterior probability? *"Confidence" intervals*
        - Intervals of defined boundary ask *how much mass?*
        - Intervals of defined mass ask *which values?*
        - *Percentile intervals* (PI): equal area in each tail (great summary for symmetric distribution, otherwise no)
        - *Highest posterior density intervals* (HPDI): narrowest interval containing mass
    - Which parameter value maximizes posterior probability? Minimizes posterior loss? *Point estimates*
    
\n
    
- You decide the question

$~$

<span style="color:brown"><span style="text-decoration:underline">**Point estimates not the point**</span>

- Don't usually want point estimates
    - Entire posterior contains more information
    - "Best" point depends upon purpose
    - Mean nearly always more sensible than mode

$~$

<span style="color:brown"><span style="text-decoration:underline">**Talking about intervals**</span>

- "Confidence" interval
    - A non-Bayesian term that doesn't even mean what it says

\n

- "Credible" interval
    - The values are not "credible" unless you trust the model and data

\n

- How about: *Compatibility interval*
    - Interval contains values compatible with the model and data as provided
    - Small world interval 

$~$

<span style="color:brown"><span style="text-decoration:underline">**Predictive checks**</span>

- Posterior probability never enough

- Even the best model might make terrible predictions

- Also want to check model assumptions

- Predictive checks: Can use samples from posterior to simulate observations
    - NB: Assumption about sampling is assumption 
    
\n

- Something like a significance test, but not

- No universally best way to evaluate adequacy of model-based predictions

- No way to justify always using a threshold like 5%

- Good predictive checks always depend upon purpose and imagination

```
Posterior predictive distribution

- Draw samples from the posterior

- Once you have samples from the posterior, you simulate multiple datasets by drawing from 
the posterior distribution multiple times. Each draw represents a possible realization of 
the parameters given the observed data.

- Generate predictions for each simulated dataset. For each simulated dataset, use the model to make predictions, generating a distribution of possible outcomes. This step essentially involves applying your model to the simulated parameter values to get simulated data.

-  Each draw from the posterior reflects uncertainty in the parameter values.

- Essentially using each draw from the posterior to simulate new data points, and the collection of 
these simulated datasets forms the posterior predictive distribution.

- The posterior predictive distribution provides a way to account for both parameter uncertainty 
(given by the posterior distribution) and variability in the observed data. It allows you to simulate 
what future data might look like based on the uncertainty captured in the posterior distribution.

- The posterior predictive distribution can be used to make predictions about future observations, 
assess model fit, and understand the range of plausible outcomes given the observed data and the 
uncertainty in your model parameters.
    
```
$~$

# Week 1: Homework

$~$

1. Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the prior distribution, using grid approximation. Use the same flat prior as before. 

```{r h1}

# Create data frame
p_grid <- seq(from = 0, to = 1, length.out = 1000)

# Create prior
prior <- rep(1, 1000)

# Calculate likelihood
likelihood <- dbinom(8, size = 15, prob = p_grid)

# Calculate unstandardized posterior
unstd.posterior <- prior * likelihood

# Standardise the posterior
posterior <- unstd.posterior/ sum(unstd.posterior)

# Plot the posterior
plot(posterior~p_grid, type = "l", xlab = "proportion of water")

```

$~$

<span style="color:grey">***Given answer***</span>

```{r h1a}

p_grid <- seq( from=0 , to=1 , length.out=1000 )
prior <- rep( 1 , 1000 )
prob_data <- dbinom( 8 , size=15 , prob=p_grid )
posterior <- prob_data * prior
posterior <- posterior / sum(posterior)
set.seed(100)
samples <- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )

# The posterior mean should be about 0.53 and the 99% percentile interval
# from 0.24 to 0.81.

```

$~$

2. Start over in 1, but now use a prior that is zero below p = 0.5 and a constant above p = 0.5. This corresponds to prior information that a majority of the Earth's surface is water. What difference does the better prior make? If it helps, compare posterior distributions (using both priors) to the true value p = 0.7.

```{r h2}

# For step prior 
# Create data frame
p_grid <- seq(from = 0, to = 1, length.out = 1000)

# Create prior
prior1 <- ifelse(p_grid < 0.5, 0, 1)

# Calculate likelihood
likelihood <- dbinom(8, size = 15, prob = p_grid)

# Calculate unstandardized posterior
unstd.posterior1 <- prior1 * likelihood

# Standardise the posterior
posterior1 <- unstd.posterior1/ sum(unstd.posterior1)

# Plot the posterior
plot(posterior1~p_grid, type = "l", xlab = "proportion of water")

# For original
# Create data frame
p_grid <- seq(from = 0, to = 1, length.out = 1000)

# Create prior
prior <- rep(1, 1000)

# Calculate likelihood
likelihood <- dbinom(8, size = 15, prob = p_grid)

# Calculate unstandardized posterior
unstd.posterior <- prior * likelihood

# Standardise the posterior
posterior <- unstd.posterior/ sum(unstd.posterior)

# Overlay the second curve
lines(posterior  ~ p_grid, col = "red", type = "l")

# Add the true p = 0.7
abline(v = 0.7, col = "blue")

# Add legend
legend("topleft", legend = c("p from step prior", "p from flat  prior", "true p value"), 
       col = c("black", "red", "blue"), lty = c(1, 1, 1))

# With the better prior, the interval of p is narrower with higher peak than the flat prior.

```

$~$

<span style="color:grey">***Given answer***</span>

```{r h2a}

p_grid <- seq( from=0 , to=1 , length.out=1000 )
prior <- c( rep( 0 , 500 ) , rep( 1 , 500 ) )
prob_data <- dbinom( 8 , size=15 , prob=p_grid )
posterior <- prob_data * prior
posterior <- posterior / sum(posterior)
set.seed(100)
samples2 <- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )

# The posterior mean should be about 0.61 and the 99% interval 0.50 to 0.82.
# This prior yields a posterior with more mass around the true value of 0.7.

dens( samples , xlab="p" , xlim=c(0,1) , ylim=c(0,6) )
dens( samples2 , add=TRUE , lty=2 )
abline( v=0.7 , col="red" )

# With the impossible values less than 0.5 ruled out, the second model piles
# up more plausibility on the higher values near the true value. The data are
# still misleading it to think that values just above 0.5 are the most plausible.
# But the posterior mean of 0.63 is much better than 0.53 from the previous
# problem.

```

$~$

3. This problem is more open-ended than the others. Feel free to collaborate on the solution. Suppose you want to estimate the Earth’s proportion of water very precisely. Specifically, you want the 99% percentile interval of the posterior distribution of p to be only 0.05 wide. This means the distance between the upper and lower bound of the interval should be 0.05. How many times will you have to toss the globe to do this? I won’t require a precise answer. I’m honestly more interested in your approach

```{r h3}

# Create data frame
p_grid <- seq(from = 0, to = 1, length.out = 1000)

# Create prior
prior <- rep(0.7, 1000)

# Calculate likelihood
likelihood <- dbinom(1400, size = 2000, prob = p_grid)

# Calculate unstandardized posterior
unstd.posterior <- prior * likelihood

# Standardise the posterior
posterior <- unstd.posterior/ sum(unstd.posterior)

# Plot the posterior
plot(posterior~p_grid, type = "l", xlab = "proportion of water")
abline(v = 0.725, col = "blue")
abline(v = 0.675, col = "red")

# The answer is 2,000 tosses if we know true value of p = 0.7. 

```

$~$

<span style="color:grey">***Given answer***</span>

```{r h3a}

# One way to approach this problem is to try a range of sample sizes and to
# plot the interval width of each. Here’s some code to compute the posterior
# and get the interval width:

set.seed(100)
N <- 20
p_true <- 0.7
W <- rbinom( 1 , size=N , prob=p_true )
p_grid <- seq( from=0 , to=1 , length.out=1000 )
prior <- rep( 1 , 1000 )
prob_data <- dbinom( W , size=N , prob=p_grid )
posterior <- prob_data * prior
posterior <- posterior / sum(posterior)
samples <- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )
PI99 <- PI( samples , 0.99 )
as.numeric( PI99[2] - PI99[1] )

# Now since we want to do this for different values of N, 
# it’s nice to make this into a function:
f <- function( N ) {
p_true <- 0.7
W <- rbinom( 1 , size=N , prob=p_true )
p_grid <- seq( from=0 , to=1 , length.out=1000 )
prior <- rep( 1 , 1000 )
prob_data <- dbinom( W , size=N , prob=p_grid )
posterior <- prob_data * prior
posterior <- posterior / sum(posterior)
samples <- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )
PI99 <- PI( samples , 0.99 )
as.numeric( PI99[2] - PI99[1] )
}

# Now if you enter f(20), you get an interval width for 20 globe tosses.
# Now notice that the interval width varies across simulations. Try f(20) a
# few times to see what I mean. But as you increase N, this variation shrinks
# rapidly. This is because as the sample size increases, the differences between
# samples shrink. So if you ignore the sample to sample variation in interval
# width, that’s okay in this example. But in the code below, I’ll account for it.

#Now we need to run simulations across a bunch of different sample size to
# find where the interval shrinks to 0.05 in width. I’ll use sapply to run 100
# simulations at each of 7 sample sizes:
Nlist <- c( 20 , 50 , 100 , 200 , 500 , 1000 , 2000 )
Nlist <- rep( Nlist , each=100 )
width <- sapply( Nlist , f )
plot( Nlist , width )
abline( h=0.05 , col="red" )

# The horizontal is sample size. The points are individual interval widths, 
# one for each simulation. The red line is drawn at a width of 0.05. 
# Looks like we need more than 2000 tosses of the globe to get the interval to be that precise.

# The above is a general feature of learning from data: The greatest returns
# on learning come early on. Each additional observation contributes less and
# less. So it takes very much effort to progressively reduce our uncertainty. So
# if your application requires a very precise estimate, be prepared to collect a
# lot of data. Or to change your approach

```












---